{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marcus/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,925\n",
      "Trainable params: 1,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,925\n",
      "Trainable params: 1,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,925\n",
      "Trainable params: 1,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 1  score for agent  3 : 1.84 at time_step: 48\n",
      "Episode: 1  score for agent  2 : 1.33 at time_step: 50\n",
      "Episode: 1  score for agent  1 : 0.82 at time_step: 51\n",
      "Episode: 101  score for agent  3 : 1.84 at time_step: 5793\n",
      "Episode: 101  score for agent  1 : 0.81 at time_step: 5798\n",
      "Episode: 101  score for agent  2 : 1.78 at time_step: 5802\n",
      "10000\n",
      "Episode: 201  score for agent  3 : 1.84 at time_step: 11484\n",
      "Episode: 201  score for agent  1 : 1.32 at time_step: 11487\n",
      "Episode: 201  score for agent  2 : 0.8 at time_step: 11490\n",
      "Episode: 301  score for agent  3 : 1.84 at time_step: 17648\n",
      "Episode: 301  score for agent  2 : 1.79 at time_step: 17658\n",
      "Episode: 301  score for agent  1 : 1.01 at time_step: 17686\n",
      "20000\n",
      "Episode: 401  score for agent  3 : 1.84 at time_step: 27891\n",
      "Episode: 401  score for agent  2 : 0.81 at time_step: 27897\n",
      "Episode: 401  score for agent  1 : 0.77 at time_step: 27901\n",
      "30000\n",
      "Episode: 501  score for agent  3 : 1.84 at time_step: 33575\n",
      "Episode: 501  score for agent  2 : 0.83 at time_step: 33577\n",
      "Episode: 501  score for agent  1 : 1.3 at time_step: 33580\n",
      "Episode: 601  score for agent  3 : 1.82 at time_step: 39809\n",
      "Episode: 601  score for agent  2 : 0.31 at time_step: 39811\n",
      "Episode: 601  score for agent  1 : 1.27 at time_step: 39815\n",
      "40000\n",
      "Episode: 701  score for agent  2 : 1.81 at time_step: 46451\n",
      "Episode: 701  score for agent  1 : 1.3 at time_step: 46453\n",
      "Episode: 701  score for agent  3 : 1.79 at time_step: 46455\n",
      "50000\n",
      "Episode: 801  score for agent  2 : 1.81 at time_step: 52489\n",
      "Episode: 801  score for agent  3 : 1.81 at time_step: 52490\n",
      "Episode: 801  score for agent  1 : -0.73 at time_step: 52494\n",
      "Episode: 901  score for agent  3 : 1.84 at time_step: 58301\n",
      "Episode: 901  score for agent  2 : 1.81 at time_step: 58307\n",
      "Episode: 901  score for agent  1 : 1.29 at time_step: 58309\n",
      "60000\n",
      "Episode: 1001  score for agent  3 : 1.84 at time_step: 64220\n",
      "Episode: 1001  score for agent  2 : 1.82 at time_step: 64224\n",
      "Episode: 1001  score for agent  1 : 1.28 at time_step: 64228\n",
      "70000\n",
      "Episode: 1101  score for agent  3 : 1.84 at time_step: 70011\n",
      "Episode: 1101  score for agent  2 : 1.82 at time_step: 70015\n",
      "Episode: 1101  score for agent  1 : 1.29 at time_step: 70018\n",
      "Episode: 1201  score for agent  3 : 1.84 at time_step: 75781\n",
      "Episode: 1201  score for agent  2 : 1.81 at time_step: 75787\n",
      "Episode: 1201  score for agent  1 : 1.28 at time_step: 75790\n",
      "80000\n",
      "Episode: 1301  score for agent  3 : 1.82 at time_step: 81835\n",
      "Episode: 1301  score for agent  2 : 1.81 at time_step: 81837\n",
      "Episode: 1301  score for agent  1 : 0.26 at time_step: 81842\n",
      "Episode: 1401  score for agent  3 : 1.84 at time_step: 87516\n",
      "Episode: 1401  score for agent  2 : 1.82 at time_step: 87520\n",
      "Episode: 1401  score for agent  1 : 1.27 at time_step: 87525\n",
      "90000\n",
      "Episode: 1501  score for agent  3 : 1.84 at time_step: 93225\n",
      "Episode: 1501  score for agent  2 : 1.81 at time_step: 93231\n",
      "Episode: 1501  score for agent  1 : 1.79 at time_step: 93233\n",
      "Episode: 1601  score for agent  3 : 1.83 at time_step: 99089\n",
      "Episode: 1601  score for agent  2 : 1.82 at time_step: 99091\n",
      "Episode: 1601  score for agent  1 : 1.76 at time_step: 99097\n",
      "100000\n",
      "Episode: 1701  score for agent  3 : 1.84 at time_step: 104824\n",
      "Episode: 1701  score for agent  2 : 1.82 at time_step: 104828\n",
      "Episode: 1701  score for agent  1 : 0.79 at time_step: 104831\n",
      "110000\n",
      "Episode: 1801  score for agent  3 : 1.84 at time_step: 110480\n",
      "Episode: 1801  score for agent  2 : 1.82 at time_step: 110484\n",
      "Episode: 1801  score for agent  1 : 0.81 at time_step: 110485\n",
      "Episode: 1901  score for agent  3 : 1.84 at time_step: 116119\n",
      "Episode: 1901  score for agent  2 : 1.81 at time_step: 116125\n",
      "Episode: 1901  score for agent  1 : 1.75 at time_step: 116131\n",
      "120000\n",
      "Episode: 2001  score for agent  3 : 1.84 at time_step: 121718\n",
      "Episode: 2001  score for agent  2 : 1.82 at time_step: 121722\n",
      "Episode: 2001  score for agent  1 : 0.81 at time_step: 121723\n",
      "Episode: 2101  score for agent  3 : 1.84 at time_step: 127302\n",
      "Episode: 2101  score for agent  2 : 1.82 at time_step: 127306\n",
      "Episode: 2101  score for agent  1 : 0.8 at time_step: 127308\n",
      "130000\n",
      "Episode: 2201  score for agent  3 : 1.84 at time_step: 132694\n",
      "Episode: 2201  score for agent  2 : 1.82 at time_step: 132698\n",
      "Episode: 2201  score for agent  1 : 0.8 at time_step: 132700\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from environment3 import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "%run environment3.py\n",
    "\n",
    "EPISODES = 2500\n",
    "number_agents = 3\n",
    "# this is RL agent for the GridWorld\n",
    "class ReinforceAgent:\n",
    "    def __init__(self, agentNr):\n",
    "        self.load_model = True\n",
    "        # actions which agent can do [l,r,u,d,ss]\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 21\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        if self.load_model and agentNr == 0:\n",
    "            self.model.load_weights('./save_model/3Agent10times10_32n_1.h5')\n",
    "        elif self.load_model and agentNr == 1:\n",
    "            self.model.load_weights('./save_model/3Agent10times10_32n_2.h5')\n",
    "        elif self.load_model and agentNr == 2:\n",
    "            self.model.load_weights('./save_model/3Agent10times10_32n_3.h5')\n",
    "\n",
    "    # state is input and probability of each action(policy) is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # create error function and training function to update policy network\n",
    "    def optimizer(self):\n",
    "        action = K.placeholder(shape=[None, 5])\n",
    "        discounted_rewards = K.placeholder(shape=[None, ])\n",
    "\n",
    "        # Calculate cross entropy error function\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "\n",
    "        # create training function\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        #optimizer = SGD(lr=0.01, clipvalue=0.5)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [],\n",
    "                                        loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # get action from policy network, see np.random.choice instructions\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0] \n",
    "\n",
    "    # calculate discounted rewards\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save states, actions and rewards for an episode\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "\n",
    "    # update policy neural network\n",
    "    def train_model(self):\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = Env()\n",
    "    \n",
    "    agents = []\n",
    "    agents.append(ReinforceAgent(0))\n",
    "    agents.append(ReinforceAgent(1))\n",
    "    agents.append(ReinforceAgent(2))\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        episodes.append(e+1)\n",
    "        done = [False]*number_agents\n",
    "        score = [0]*number_agents\n",
    "        # fresh env\n",
    "        state = [env.reset(0), env.reset(1), env.reset(2)]\n",
    "        state[0] = np.reshape(state[0], [1, 21])\n",
    "        state[1] = np.reshape(state[1], [1, 21])\n",
    "        state[2] = np.reshape(state[2], [1, 21])\n",
    "\n",
    "        while (done[0] == False or done[1] == False or done[2] == False):\n",
    "            for agent in agents:\n",
    "                #t0 = time.time()\n",
    "                #start = time.time()\n",
    "                i = agents.index(agent)\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                global_step += 1\n",
    "                if global_step % 10000 == 0:\n",
    "                    print(global_step)\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state[i])\n",
    "                next_state, reward, done[i] = env.step(action, i)\n",
    "                next_state = np.reshape(next_state, [1, 21])\n",
    "\n",
    "                agent.append_sample(state[i], action, reward)\n",
    "                score[i] += reward\n",
    "                state[i] = copy.deepcopy(next_state)\n",
    "                \n",
    "                #end = time.time()\n",
    "                #print(end-start)\n",
    "                \n",
    "                if done[i]:\n",
    "                # update policy neural network for each episode\n",
    "                    agent.train_model()\n",
    "                    #scores.append(score)\n",
    "                    #episodes.append(e)\n",
    "                    score[i] = round(score[i], 2)\n",
    "                    if e % 100 == 0:\n",
    "                        print(\"Episode:\", e + 1, \" score for agent \",i+1,\":\",score[i], \"at time_step:\", global_step)\n",
    "                t1 = time.time()\n",
    "                #print(t1-t0)\n",
    "        \n",
    "        scores.append(score)\n",
    "        if e  % 100 == 0:\n",
    "    \n",
    "            #agents[0].model.save_weights(\"./save_model/reinforce_new1.h5\")\n",
    "            #agents[1].model.save_weights(\"./save_model/reinforce_new2.h5\")\n",
    "            #agents[2].model.save_weights(\"./save_model/reinforce_new3.h5\")\n",
    "            if e ==2500:\n",
    "                close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pickle\n",
    "\n",
    "with open ('./save_data/episodes', 'rb') as fp:\n",
    "    episodes = pickle.load(fp)\n",
    "with open ('./save_data/3Agent10times10_32n_report', 'rb') as fp:\n",
    "    scores = pickle.load(fp)\n",
    "    \n",
    "s1 = []\n",
    "s2 = []\n",
    "s3 = []\n",
    "for i in range(0, 2500):\n",
    "    s1.append(scores[i][0])\n",
    "    s2.append(scores[i][1])\n",
    "    s3.append(scores[i][2])\n",
    "    \n",
    "#s2_new = [i * 1.43 for i in s2]\n",
    "\n",
    "pylab.plot(episodes, s1, 'b', linewidth=0.1)\n",
    "pylab.plot(episodes, s2, 'g', linewidth=0.1)\n",
    "pylab.plot(episodes, s3, 'r', linewidth=0.1)\n",
    "pylab.xlabel('Episodes')\n",
    "pylab.ylabel('Reward')\n",
    "pylab.ylim((-20, 3))\n",
    "#pylab.xlim((0, 2500))\n",
    "#pylab.title('Reward as function of episodes')\n",
    "pylab.legend(('Agent 1', 'Agent 2', 'Agent 3'),\n",
    "           loc='lower right')\n",
    "\n",
    "pylab.savefig('./save_graph/3Agent10times10_32n_143864steps', format='eps', dpi=900)\n",
    "#pylab.savefig('./save_graph/test2', format='eps', dpi=900)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/writing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./save_data/3Agent10times10_32n_report', 'wb') as fp:\n",
    "    pickle.dump(scores, fp)\n",
    "#with open ('./save_data/test', 'rb') as fp:\n",
    "   # list_name = pickle.load(fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('./save_data/3Agent10times10notseed', 'rb') as fp:\n",
    "    scores = pickle.load(fp)\n",
    "\n",
    "s1 = []\n",
    "s2 = []\n",
    "s3 = []\n",
    "for i in range(0, 2500):\n",
    "    s1.append(scores[i][0])\n",
    "    s2.append(scores[i][1])\n",
    "    s3.append(scores[i][2])\n",
    "    \n",
    "print(max(s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[0].model.save_weights(\"./save_model/3Agent10times10_32n_1.h5\")\n",
    "agents[1].model.save_weights(\"./save_model/3Agent10times10_32n_2.h5\")\n",
    "agents[2].model.save_weights(\"./save_model/3Agent10times10_32n_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

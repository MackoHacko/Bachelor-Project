{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 24)                384       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 1,109\n",
      "Trainable params: 1,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-526a1fb47a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# get action for the current state and go one step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;31m#next_state.extend((0,0,0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiT/KEX/PingiDense/Policy Gradient/envorig.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mnext_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiT/KEX/PingiDense/Policy Gradient/envorig.py\u001b[0m in \u001b[0;36mmove_rewards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mnew_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mnew_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiT/KEX/PingiDense/Policy Gradient/envorig.py\u001b[0m in \u001b[0;36mmove_const\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmove_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbase_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2461\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[1;32m   2462\u001b[0m                            self.tk.splitlist(\n\u001b[0;32m-> 2463\u001b[0;31m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[1;32m   2464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m         \u001b[0;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import numpy as np\n",
    "from envorig import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "%run envorig.py\n",
    "\n",
    "EPISODES = 2500\n",
    "\n",
    "# this is REINFORCE Agent for GridWorld\n",
    "class ReinforceAgent:\n",
    "    def __init__(self):\n",
    "        self.load_model = False\n",
    "        # actions which agent can do\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 15\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights('./save_model/2Agent10times10_1.h5')\n",
    "\n",
    "    # state is input and probability of each action(policy) is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # create error function and training function to update policy network\n",
    "    def optimizer(self):\n",
    "        action = K.placeholder(shape=[None, 5])\n",
    "        discounted_rewards = K.placeholder(shape=[None, ])\n",
    "\n",
    "        # Calculate cross entropy error function\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "\n",
    "        # create training function\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [],\n",
    "                                        loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # get action from policy network, see np.random.choice instructions\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0] \n",
    "\n",
    "    # calculate discounted rewards\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save states, actions and rewards for an episode\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "\n",
    "    # update policy neural network\n",
    "    def train_model(self):\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = ReinforceAgent()\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        # fresh env\n",
    "        state = env.reset()\n",
    "        #state.extend((0,0,0))\n",
    "        state = np.reshape(state, [1, 15])\n",
    "\n",
    "        while not done:\n",
    "            #start = time.time()\n",
    "            global_step += 1\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            #next_state.extend((0,0,0))\n",
    "            next_state = np.reshape(next_state, [1, 15])\n",
    "\n",
    "            agent.append_sample(state, action, reward)\n",
    "            score += reward\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            #end = time.time()\n",
    "            #print(end-start)\n",
    "            \n",
    "            if done:\n",
    "                # update policy neural network for each episode\n",
    "                agent.train_model()\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                score = round(score, 2)\n",
    "                if e % 100 == 0:\n",
    "                    print(\"episode:\", e, \"  score:\", score, \"  time_step:\",\n",
    "                          global_step)\n",
    "        if e == 2499:\n",
    "            agent\n",
    "        if e % 100 == 0:\n",
    "            #pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/1Agent10times10.eps\")\n",
    "            agent.model.save_weights(\"./save_model/try2AgentUR.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pickle\n",
    "\n",
    "with open ('./save_data/1Agent10times10_24n_seed3', 'rb') as fp:\n",
    "   s = pickle.load(fp)\n",
    "with open ('./save_data/episodes', 'rb') as fp:\n",
    "   episodes = pickle.load(fp)\n",
    "\n",
    "pylab.plot(episodes, s, 'b', linewidth=0.1)\n",
    "\n",
    "pylab.xlabel('Episodes')\n",
    "pylab.ylabel('Reward')\n",
    "pylab.ylim((-20, 0))\n",
    "#pylab.title('Reward as function of episodes (Total steps = 27984)')\n",
    "\n",
    "pylab.savefig(\"./save_graph/1Agent10times10_24n_seed3_27984step.eps\", format='eps', dpi=900)\n",
    "\n",
    "print(min(s))\n",
    "print(max(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/load lists via pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./save_data/1Agent10times10_24n_seed3', 'wb') as fp:\n",
    "    pickle.dump(scores, fp)\n",
    "#with open ('./save_data/1Agent10times10', 'rb') as fp:\n",
    "   #scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test = agent.model.get_weights()\n",
    "print(test)\n",
    "\n",
    "with open('1Agent10times10_24n_L', 'wb') as fp:\n",
    "    pickle.dump(test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

\relax 
\citation{website123}
\citation{silver}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries and background}{}}
\newlabel{prelback}{{II}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Single agent reinforcement learning}{}}
\newlabel{eq:2a}{{2a}{}}
\newlabel{eq:2b}{{2b}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Neural networks}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a single neuron taking three inputs and mapping them to a single output through an activation function $f$.}}{}}
\newlabel{singleneuron}{{1}{}}
\newlabel{eq:3a}{{3a}{}}
\newlabel{eq:3b}{{3b}{}}
\newlabel{eq:3c}{{3c}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Different types of activation functions used in neural networks.}}{}}
\newlabel{actfunc}{{2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A neural network with one hidden layer.}}{}}
\newlabel{nnonelayer}{{3}{}}
\citation{BP}
\citation{code}
\newlabel{5}{{5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Deep reinforcement learning}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methods}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Environment and rewards}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The visualized environment with three agents in it, crosses represent horizontally moving obstacles and squares are stationary goals.}}{}}
\newlabel{fig_sim}{{4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Distribution of rewards that agents could receive from taking an action that transitions it to a different state.}}{}}
\newlabel{tab:table1}{{I}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Algorithm details}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Main Results}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Results from 2500 episodes one agent, two hidden layer with 24 neurons. The maximum reward was 0.3.}}{}}
\newlabel{1agent24n}{{5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Results from 2500 episodes one agent, two hidden layer with 24 neurons. The maximum reward was 0.3.}}{}}
\newlabel{1agent32n}{{6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Results from 2500 episodes two agents, two hidden layer with 24 neurons. The maximum reward for the agents were [0.2, 0.3].}}{}}
\newlabel{2agent24n}{{7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Results from 2500 episodes two agents, two hidden layer with 32 neurons. The maximum reward for the agents were [0.2, 0.3].}}{}}
\newlabel{2agent32n}{{8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Results from 2500 episodes thee agents, two hidden layer with 32 neurons. The maximum reward for the agents were [0.2, 0.3, 0.4].}}{}}
\newlabel{3agent32n}{{9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Results from 2500 episodes four agents, two hidden layer with 32 neurons. The maximum reward for the agents were [0.2, 0.3, 0.4, 0.4].}}{}}
\newlabel{4agent32n}{{10}{}}
\citation{Reward}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Average performance/computational time for a learning sequence of one to four agents.}}{}}
\newlabel{perf}{{II}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Average number of collisions per 2000 episodes evaluated over ten runs with learned policies.}}{}}
\newlabel{perf}{{III}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Data Analysis}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Learning sequences}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Four agent environment, the reference line is at -1.7 on the reward axis and corresponds to the agents going the fastest way to goal with two collisions on the way.}}{}}
\newlabel{selfish}{{11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Evaluation of Learned Policies}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-D}}Generality and Model Analysis}{}}
\citation{epsilon}
\citation{Qlearn}
\citation{Conv}
\citation{Reward}
\bibstyle{IEEEtran}
\bibdata{./KEX_bib_example}
\bibcite{website123}{1}
\bibcite{silver}{2}
\bibcite{Sutton:1998:IRL:551283}{3}
\bibcite{BP}{4}
\bibcite{code}{5}
\bibcite{Reward}{6}
\bibcite{epsilon}{7}
\bibcite{Qlearn}{8}
\bibcite{Conv}{9}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-E}}Future Work}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{}}
\@writefile{toc}{\contentsline {section}{References}{}}

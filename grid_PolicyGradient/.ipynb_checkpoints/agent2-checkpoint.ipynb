{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marcus/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                456       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 1,181\n",
      "Trainable params: 1,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                456       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 1,181\n",
      "Trainable params: 1,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0  score for agent  1 : -3063.6 at time_step: 49851\n",
      "Episode: 0  score for agent  2 : -3717.1 at time_step: 54597\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import numpy as np\n",
    "import time\n",
    "from env2 import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "%run env2.py\n",
    "\n",
    "EPISODES = 2500\n",
    "\n",
    "# this is REINFORCE Agent for GridWorld\n",
    "class ReinforceAgent:\n",
    "    def __init__(self):\n",
    "        self.load_model = False\n",
    "        # actions which agent can do\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 18\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights('./save_model/reinforce_trained.h5')\n",
    "\n",
    "    # state is input and probability of each action(policy) is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # create error function and training function to update policy network\n",
    "    def optimizer(self):\n",
    "        action = K.placeholder(shape=[None, 5])\n",
    "        discounted_rewards = K.placeholder(shape=[None, ])\n",
    "\n",
    "        # Calculate cross entropy error function\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "\n",
    "        # create training function\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [],\n",
    "                                        loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # get action from policy network, see np.random.choice instructions\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0] \n",
    "\n",
    "    # calculate discounted rewards\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save states, actions and rewards for an episode\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "\n",
    "    # update policy neural network\n",
    "    def train_model(self):\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = Env()\n",
    "    \n",
    "    agents = []\n",
    "    agents.append(ReinforceAgent())\n",
    "    agents.append(ReinforceAgent())\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = [False]*2\n",
    "        score = [0]*2\n",
    "        # fresh env\n",
    "        state = [env.reset(0), env.reset(1)]\n",
    "\n",
    "        state[0] = np.reshape(state[0], [1, 18])\n",
    "        state[1] = np.reshape(state[1], [1, 18])\n",
    "\n",
    "        while (done[0] == False or done[1] == False):\n",
    "            for agent in agents:\n",
    "                start = time.time()\n",
    "                i = agents.index(agent)\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                global_step += 1\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state[i])\n",
    "                next_state, reward, done[i] = env.step(action, i)\n",
    "                next_state = np.reshape(next_state, [1, 18])\n",
    "\n",
    "                agent.append_sample(state[i], action, reward)\n",
    "                score[i] += reward\n",
    "                state[i] = copy.deepcopy(next_state)\n",
    "                \n",
    "                end = time.time()\n",
    "                #print(end-start)\n",
    "                \n",
    "                if done[i]:\n",
    "                # update policy neural network for each episode\n",
    "                    agent.train_model()\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    score[i] = round(score[i], 2)\n",
    "                    print(\"Episode:\", e, \" score for agent \",i+1,\":\",score[i], \"at time_step:\", global_step)\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            #pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/reinforce.png\")\n",
    "            agent.model.save_weights(\"./save_model/reinforce.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 µs ± 7.43 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(18,24)\n",
    "b = np.random.rand(24,5)\n",
    "\n",
    "%timeit np.dot(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
